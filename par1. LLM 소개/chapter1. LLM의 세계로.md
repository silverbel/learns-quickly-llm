
# LLM 정의

(반드시는 아니지만) **트랜스포머 아키텍처**에서 파생된 AI 모델로 언어, 코드 등을 구문 분석하고 생성하기 위해 설계

### 트랜스포머 아키텍처란?
- 2017년 구글이 발표한 "Attention is All You Need" 논문에서 소개된 딥러닝 아키텍처입니다. 현재 GPT, Claude, BERT 등 거의 모든 대규모 언어 모델의 기반이 되는 핵심 기술.

#### 트랜스포머 아키텍처의 주요 구성요소

책에서는 LLM과 트랜스포머가 해결하고 있는 NLP(national language processing - 자연어 처리) 작업에 대해 먼저 설명.
NLP의 하위 분야인 언어 모델링 부터 설명.

## 언어 모델링
### 자동 인코딩
- 알려진 어휘에서 문장의 어느 부분이든 누락된 단어를 채우도록 모델에 요청합니다.
- 앞서 온 토큰의 내용만으로, 다음 토큰을 예측하도록 훈련
- 트렌스포머 모델의 디코더 부분!
- 이전 토큰을 기바으로 문장의 다음 토큰을 예측. 주어진 맥락을 따라서 일관성 있는 텍스트를 생성하는데 효과적
- 예시) GPT

### 자기회귀
- 알려진 어휘에서 주어진 문장의 바로 다음에 가장 가능성 있는 토큰을 생성하도록 모델에 요청합니다.
- 원래의 문장을 재구성하도록 훈련
- 트랜스포머 모델의 인코더 부분!
- 입력 토큰중 일부를 가리고 남아있는 토큰으로부터 그것들을 예측하여 컨텍스트를 양방향으로 이해하여 표현을 구축. 토큰간의 맥락을 빠르고 대규모로 포착하는데 능숙하여 텍스틑 분류작업에 좋음
- 예시) BERT

### 자기회귀 + 자동 인코딩
- 다양하고 유연한 텍스트를 생성하기 위해 인코더와 디코터를 함께 사용
- 인코더를 사용하여 추가 맥락을 포착하는 능력 때문에 순수한 디코더 기반의 자기회귀보다 여러가지 컨텍스트에서 더 다양하고 창의적인 텍스트 생성 가능
- 예시) T5

📍 요약하면 LLM은 다음과 같은 언어모델.
- 자기회기
- 자동 인코딩
- 자기회기 + 자동 인코딩


## LLM 작동 원리

모든 LLM은 컨텍스트에 신경을 씁니다. 목표는 입력 테스트의 다른 토큰과 관련하여 각 토큰을 이해하는 것입니다.
```
예시) 문장마다 python의 의미가 다르다
나는 나의 반려뱀 python을 좋아합니다. vs 나는 python으로 코딩하는 것을 좋아합니다.
```
- 사전훈련
- 전이학습
- 미세조정
- 어텐션
- 임베딩
- 토큰화
- 언어 모델링을 넘어서: 정렬 + RLHF
- 도메인 특화 LLM
