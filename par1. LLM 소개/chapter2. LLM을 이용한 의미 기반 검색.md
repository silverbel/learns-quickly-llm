# CHAPTER 2. LLM을 이용한 의미 기반 검색

## 비대칭적 의미 기반 검색

### 의미 기반 검색 시스템
- 사용자 쿼리의 의미와 맥락을 이해하고, 이를 검색 가능한 문서들의 의미 및 맥락과 비교하여 적절한 결과를 찾아줌.

### 비대칭 부분
- 입력 쿼리의 의미정보(기본적으로 크기)와 검색 시스템이 검색해야 하는 문서/정보 사이에 불균형이 있다는 사실을 의미.

### 비대칭적 기반 검색 시스템
- 쿼리에 정확한 단어를 사용하지 않더라도 정확한 검색결과를 얻을수 있음.
- 사용자가 검색해야 하는 단어를 매우 정확히 알고 있다고 가정하기보다는 LLM의 학습에 의존.

### 비대칭적 기반 검색 시스템의 단점
- 대문자 or 구두점의 차이와 같은 텍스트의 작은 변화에 지나치게 민감.
- 현지화된 문화 지식에 의존하는 풍자나 아이러니 같은 애매한 개념에 어려움을 겪음.
- 기존 방식보다 구현하고 유지하는데 더 많은 계산 비용이 듬. 특시 오픈소스 구성요소를 많이 가지고 자체적으로 개발한 시스템일수록 더욱 비용이 듬.


## 솔루션 개요

비대칭적 의미 기반 검색 시스템의 흐름

1단계 : 문서저장
- 임베딩을 위한 문서 저장
- 의미 정보를 인코딩하기 위한 텍스트 임베딩 생성
- 나중에 쿼리가 주어졌을때 검색할 수 있도록 임베딩을 데이터베이스에 저장

2단계 : 문서 검색
- 사용자에게 전처리 되고 정리 할 수 있는 쿼리 수행.
- 임베딩 유사도를 통해 후보문서를 검색.
- 필요한 경우 후보문서의 순위를 재순위화.
- 최종검색 결과를 사용자에게 반환

## 구성요소 

### 텍스트 임베더
- 의미 기반 검색 시스템의 핵심
- 텍스트 문서 or 단어 or 구문 -> 벡터로 변환
- 이 벡터는 입력된 텍스트마다 고유하며 구문의 맥락적 의미를 포착해야 함.
- 즉, 텍스트 임베더의 선택은 텍스트를 벡터로 표현하는 품질을 결정! (매우 중요!)
ex> 오픈 AI의 "임베딩" 제품 (클로즈드 소스)

#### 무엇이 텍스트를 유사하게 만드는가?

##### 코사인 유사도
- 텍스트를 벡터로 변환하면, 텍스트 조각끼리의 유사여부를 파악하기 위한 수학적 표현, 두 벡터가 얼마나 유사한지 측정하는 방법
```
두 벡터사이의 각도를 살펴보고, 방향이 얼마나 가까운지에 따라 점수를 매김 (벡터의 크기는 중요하지 않고, 방향만 중요!)
- 벡터가 정확히 같은 방향이면 : 1점
- 그들이 수직이면 (90도간격) : 0점
- 그들이 반대방향이면        : -1점
```
[예시사진2]


##### 오픈AI의 임베딩 엔진

- 오픈 AI에서 임베딩을 얻는 것은 몇줄의 코드를 작성하는것으로 가능
- 전체 시스템의 의미적으로 유사한 항목을 서로 가까이 배치하는 임베딩 메커니즘에 의존
- 항목이 실제로 유사하면 코사인유사도가 큼

``` 오픈 AI에서 텍스트 임베딩 얻기
# 스크립트를 실행하기 위해 필요한 모듈 가져오기
from openai import OpenAI

# 환경 변수 'OPENAI_API_KEY'에 저장된 값을 사용하여 오픈AI API 키 설정하기
client = OpenAI(
  api_key = os.environ.get("OPENAI_API_KEY")
)

#텍스트 임베딩에 사용될 엔진 설정하기
ENGINE = 'text-embedding-3-large' # 벡터 크기 3072

#지정된 엔진을 사용하여 주어진 텍스트의 벡터 표현 생성하기
def get_embeddings(texts, engine=ENGINE):
  response = client.embeddings.create(
    input=texts,
    model=engine
  )

  return [d.embeddings for d in list(response.data)]

embedded_text = get_embeddings('I love to be vectorized', engine=ENGINE)

# 결과 벡터의 길이를 확인하여 예상 크기(3072)와 일치하는지 확인하기
len(embedded_text) == '3072'

```


#### 오픈 소스 대안
오픈AI와 여러 회사가 강력한 텍스트 입데이 제품을 제공하고 있지만, 텍스트 임베딩을 위해 여러 오픈소스 대안들도 존재.

##### 바이인코더 (bi-encoder)
- 자동 인코딩 LLM중 하나인 BERT를 이용한 모델, 두개의 BERT모델을 훈련
- 하나는 입력텍스트를 인코딩, 다른하나는 출력 텍스트를 인코딩
- 문서간의 유사성을 학습하기 위해 단일 LLM(BERT)을 두개로 복제하여 병렬로 학습하는 독특한 방식으로 훈련됨
- 예를 들어, 바이인코더는 질문을 단락에 연결하여 벡터공간에서 서로 가깝게 표시되도록 학습 가능.
[예시사진 3]

``` 사전 훈련된 sentence_transformer 패키지의 바이 인코더를 사용하여 텍스트를 임베딩하는 예시
# 문장 트랜스포머 라이브러리 임포트하기
from sentence_transformers import SentenceTransformer

# 'all-mpnet-base-v2' 사전 훈련 모델로 SentenceTransformer 초기화하기
model = SentenceTransformer(
  'sentence=transformers/all-mpnet-base-v2'

# 임베딩을 생성할 문서의 리스트 정의하기
docs = [
        "Around 9 million people live in London",
        "London is knonwn for its financial district"
      ]

# 문서들에 대한 벡터 임베딩 생성하기 -> doc_emb에 저장
doc_emb = model.encode(
    docs,                     # 문서들 (반복 가능한(iterable) 문자열) 
    batch_size = 32           # 이 크기로 임베딩을 일괄 처리
    show_progress_bar = True  # 진행 막대를 표시
)

# 임베딩의 형태는 (2, 768)이며, 이는 길이가 768이고
# 생성된 임베딩이 2개임을 나타냅니다
doc_emb.shape # == (2, 768)
```

##### ❓ batch_size = 32 는 무엇일까
```
한 번에 GPU/CPU로 처리할 문서의 개수를 의미! (OOM 방지)

# 환경별 권장 batch_size
- CPU 환경: 8-16
- GPU (8GB): 32-64
- GPU (16GB+): 64-128
```


### 문서 청킹



### 벡터 데이터베이스
### 검색 결과 재순위화
### API

통합
- 성능

클로즈드 소스 구성 요소의 비용
