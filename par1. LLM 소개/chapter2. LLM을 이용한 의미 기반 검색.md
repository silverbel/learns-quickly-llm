# CHAPTER 2. LLM을 이용한 의미 기반 검색

<img width="407" height="385" alt="image" src="https://github.com/user-attachments/assets/7e066cfc-4298-42db-aa26-03b1b2dca376" />

## 비대칭적 의미 기반 검색

### 의미 기반 검색 시스템
- 사용자 쿼리의 의미와 맥락을 이해하고, 이를 검색 가능한 문서들의 의미 및 맥락과 비교하여 적절한 결과를 찾아줌.

### 비대칭 부분
- 입력 쿼리의 의미정보(기본적으로 크기)와 검색 시스템이 검색해야 하는 문서/정보 사이에 불균형이 있다는 사실을 의미.

### 비대칭적 기반 검색 시스템
- 쿼리에 정확한 단어를 사용하지 않더라도 정확한 검색결과를 얻을수 있음.
- 사용자가 검색해야 하는 단어를 매우 정확히 알고 있다고 가정하기보다는 LLM의 학습에 의존.

### 비대칭적 기반 검색 시스템의 단점
- 대문자 or 구두점의 차이와 같은 텍스트의 작은 변화에 지나치게 민감.
- 현지화된 문화 지식에 의존하는 풍자나 아이러니 같은 애매한 개념에 어려움을 겪음.
- 기존 방식보다 구현하고 유지하는데 더 많은 계산 비용이 듬. 특시 오픈소스 구성요소를 많이 가지고 자체적으로 개발한 시스템일수록 더욱 비용이 듬.


## 솔루션 개요

비대칭적 의미 기반 검색 시스템의 흐름

1단계 : 문서저장
- 임베딩을 위한 문서 저장
- 의미 정보를 인코딩하기 위한 텍스트 임베딩 생성
- 나중에 쿼리가 주어졌을때 검색할 수 있도록 임베딩을 데이터베이스에 저장

2단계 : 문서 검색
- 사용자에게 전처리 되고 정리 할 수 있는 쿼리 수행.
- 임베딩 유사도를 통해 후보문서를 검색.
- 필요한 경우 후보문서의 순위를 재순위화.
- 최종검색 결과를 사용자에게 반환

## 구성요소 

### 텍스트 임베더
- 의미 기반 검색 시스템의 핵심
- 텍스트 문서 or 단어 or 구문 -> 벡터로 변환
- 이 벡터는 입력된 텍스트마다 고유하며 구문의 맥락적 의미를 포착해야 함.
- 즉, 텍스트 임베더의 선택은 텍스트를 벡터로 표현하는 품질을 결정! (매우 중요!)
ex> 오픈 AI의 "임베딩" 제품 (클로즈드 소스)

#### 무엇이 텍스트를 유사하게 만드는가?

##### 코사인 유사도
- 텍스트를 벡터로 변환하면, 텍스트 조각끼리의 유사여부를 파악하기 위한 수학적 표현, 두 벡터가 얼마나 유사한지 측정하는 방법
```
두 벡터사이의 각도를 살펴보고, 방향이 얼마나 가까운지에 따라 점수를 매김 (벡터의 크기는 중요하지 않고, 방향만 중요!)
- 벡터가 정확히 같은 방향이면 : 1점
- 그들이 수직이면 (90도간격) : 0점
- 그들이 반대방향이면        : -1점
```
<img width="407" height="385" alt="image" src="https://github.com/user-attachments/assets/c194fa72-2f5a-42d3-a6d6-64a829a604c0" />



##### 오픈AI의 임베딩 엔진

- 오픈 AI에서 임베딩을 얻는 것은 몇줄의 코드를 작성하는것으로 가능
- 전체 시스템의 의미적으로 유사한 항목을 서로 가까이 배치하는 임베딩 메커니즘에 의존
- 항목이 실제로 유사하면 코사인유사도가 큼

``` 오픈 AI에서 텍스트 임베딩 얻기
# 스크립트를 실행하기 위해 필요한 모듈 가져오기
from openai import OpenAI

# 환경 변수 'OPENAI_API_KEY'에 저장된 값을 사용하여 오픈AI API 키 설정하기
client = OpenAI(
  api_key = os.environ.get("OPENAI_API_KEY")
)

#텍스트 임베딩에 사용될 엔진 설정하기
ENGINE = 'text-embedding-3-large' # 벡터 크기 3072

#지정된 엔진을 사용하여 주어진 텍스트의 벡터 표현 생성하기
def get_embeddings(texts, engine=ENGINE):
  response = client.embeddings.create(
    input=texts,
    model=engine
  )

  return [d.embeddings for d in list(response.data)]

embedded_text = get_embeddings('I love to be vectorized', engine=ENGINE)

# 결과 벡터의 길이를 확인하여 예상 크기(3072)와 일치하는지 확인하기
len(embedded_text) == '3072'

```


#### 오픈 소스 대안
오픈AI와 여러 회사가 강력한 텍스트 입데이 제품을 제공하고 있지만, 텍스트 임베딩을 위해 여러 오픈소스 대안들도 존재.

##### 바이인코더 (bi-encoder)
- 자동 인코딩 LLM중 하나인 BERT를 이용한 모델, 두개의 BERT모델을 훈련
- 하나는 입력텍스트를 인코딩, 다른하나는 출력 텍스트를 인코딩
- 문서간의 유사성을 학습하기 위해 단일 LLM(BERT)을 두개로 복제하여 병렬로 학습하는 독특한 방식으로 훈련됨
- 예를 들어, 바이인코더는 질문을 단락에 연결하여 벡터공간에서 서로 가깝게 표시되도록 학습 가능.
<img width="407" height="385" alt="image" src="https://github.com/user-attachments/assets/9e4b2f17-4a68-45b8-bd39-c5b7f96a2114" />


``` 사전 훈련된 sentence_transformer 패키지의 바이 인코더를 사용하여 텍스트를 임베딩하는 예시
# 문장 트랜스포머 라이브러리 임포트하기
from sentence_transformers import SentenceTransformer

# 'all-mpnet-base-v2' 사전 훈련 모델로 SentenceTransformer 초기화하기
model = SentenceTransformer(
  'sentence=transformers/all-mpnet-base-v2'

# 임베딩을 생성할 문서의 리스트 정의하기
docs = [
        "Around 9 million people live in London",
        "London is knonwn for its financial district"
      ]

# 문서들에 대한 벡터 임베딩 생성하기 -> doc_emb에 저장
doc_emb = model.encode(
    docs,                     # 문서들 (반복 가능한(iterable) 문자열) 
    batch_size = 32           # 이 크기로 임베딩을 일괄 처리
    show_progress_bar = True  # 진행 막대를 표시
)

# 임베딩의 형태는 (2, 768)이며, 이는 길이가 768이고
# 생성된 임베딩이 2개임을 나타냅니다
doc_emb.shape # == (2, 768)
```

##### ❓ batch_size = 32 는 무엇일까
```
한 번에 GPU/CPU로 처리할 문서의 개수를 의미! (OOM 방지)

# 환경별 권장 batch_size
- CPU 환경: 8-16
- GPU (8GB): 32-64
- GPU (16GB+): 64-128
```


### 문서 청킹
연구 문서와 같은 큰 문서를 다룰 때 임베딩하는 어려움이 있다. 전체 문서를 단일 벡터로 임베딩하는것은 종종 실용적이지 않다.
이 문제에 대한 해결책은 문서 청킹을 사용한 것!

❓ 문서청킹 이란 : 큰 문서를 임베딩하기 위해 더 작고 관리 가능한 청크로 나누는 것을 의미

#### 최대 토큰 번위 분할
- 문서 청킹에 대한 한가지 접근 방법
- 구현하기 가장 쉬운 방법
- 우려사항중 하나는 중요한 텍스트 일부를 나눠진 청크 사이에서 실수로 잘라낼수 있다. 이문제를 보완하기 위해 지정된 양의 토큰으로 겹치는 범위를 설정. (중복 느낌을 주지만, 더 높은 정확도와 대기시간 기대)
- 정보가 청크 사이에 나누어지거나 중복된 정보가 있는 청크가 생길수 있어 검색시스템을 혼란스럽게 할수 있다.

#### 맞춤형 구분 기호 찾기
청킹 방법을 돕기 위해 PDF에서의 페이지 분리나 단락 사이의 새로운 줄과 같은 맞춤형 자연 구분기호를 찾을수 있다. 주어진 문서에 대해 텍스트 내의 자연스러운 공백을 식별하고 이를 사용하여 결국 임베딩되는 청크에 들어갈 더 의미있는 텍스트 단위를 생성하게 됨.
- 자연 공백을 사용하는 청킹은 일반적으로 균일하지 않는 청크 크기를 갖게 됩니다.

청크를 구성하는 방식에 있어서 좀더 창의적으로 접근하기 위해, 머신러닝을 활용할수도 있다.

#### 클러스터링을 사용하여 의미 기반 문서 생성하기
- 의미적으로 유사한 작은 청크를 결합하여 새로운 문서를 생성
  <img width="634" height="401" alt="image" src="https://github.com/user-attachments/assets/1cef6739-825f-48fa-8739-356d61cf4507" />
- 내용의 일부가 주변 텍스트와 맥락에서 벗어나는 단점이 존재. 이 방법은 청크들이 서로 관련이 없을때(독립적일때) 잘 동작!

#### 청크로 나누지 않고 전체 문서 사용하기
- 텍스트를 임베딩할때 컨텍스트 윈도우 한계에 도달할 수 있다는 단점이 존재

결국 문서의 장단점을 고려하여 문서 청킹을 방법을 고민해야한다.
<img width="613" height="437" alt="image" src="https://github.com/user-attachments/assets/85f797a3-94c7-460e-9078-484ff5a58b9d" />


### 벡터 데이터베이스
유사도 검색(similarity search)을 수행하는 특화 DB
임베딩을 벡터 데이터베이스에 저장하게되면 의미적으로 유사한 텍스트를 검색하는 최근접 이웃 탐색을 효율적으로 수행가능

``` Q: 벡터 데이터베이스 종류에 대해 알려줘
주요 벡터 DB 분류:

1. 전용 벡터 DB (Purpose-built)

- Pinecone: 완전 관리형 SaaS, 가장 쉬운 시작, 비용은 높음
- Weaviate: 오픈소스, GraphQL 지원, 하이브리드 검색 강점
- Milvus: 오픈소스, 대규모 처리에 강함, Kubernetes 네이티브
- Qdrant: Rust 기반, 고성능, 필터링 기능 우수

2. 기존 DB의 벡터 확장

- PostgreSQL + pgvector: 익숙한 SQL, 소규모~중규모 적합
- - Redis Vector Search: 인메모리 속도, 캐싱과 함께 사용
- Elasticsearch: 하이브리드 검색(키워드+벡터) 강점
- MongoDB Atlas Vector Search: 문서DB + 벡터 검색
```


### 검색 결과 재순위화



### API

통합
- 성능

클로즈드 소스 구성 요소의 비용
